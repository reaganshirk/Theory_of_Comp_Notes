* He's talking about a paper he wrote right now, I don't really know what's happening
	* I think he's talking about the delta definition for a
	turing machine?
	* He has a turing machine diagram pulled up but, once again,
	I don't really know what's happening
	* I couldn't find this paper on his website so if anyone has
	the link lmk
* I don't think anyone is really taking notes today sorry friends
* Ah he's putting the projector screens up I guess it's time to
pay attention
* Encoding
	* We want to have some way of "standardizing" the input
	* If we have some object O we can denote its encoding with
	<O>
		* You can encode a graph (G =(V, E)) with 4 verticies
		(that I can draw later) as
			* <G> = ({1,2,3,4}, {(1,2),(1,3),(2,3),(3,4)})
			* We will not go into extreme details of the 
			representation
			* We will not go all the way down to the lowest
			level	
	* If M is a description of a turing machine, then <M> is the
	encoding of M
	* Eventually we will be passing <M, w> as input
* Variants of Turing Machines
	* Is the tape infinite in one or both directions
	* Multiple tapes
	* Non-deterministic
	* How to handle turing machines with S options?
	* How can we simulate a multi-tape turing machine with a
	turing machine that has just 1 tape?
	* People started asking smart questions and I zoned out
* Non deterministic turing machines
	* The turing machine accepts if some branch of the
	computation leads to an accept state
	* delta is Q x Gamma -> P(Q x Gamma x {L, R})
	* Theroem: every nondeterministic turing machine has an
	equivalent deterministic during machine
	* Proof idea: something with a pyramid..? need a bfs though
		* I can't see what he's writing so I'm gonna guess on
		a lot of these words
		* Enumerate all the computation nodes
			* need some encoding in terms of the transition
			functions
		* Look at the path leading to such a computation node